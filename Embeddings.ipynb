{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc1a7302-5289-4c8b-be95-87c0ace4629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_addons as tfa\n",
    "import wandb\n",
    "\n",
    "import settransformer as sf\n",
    "import tf_utils as tfu\n",
    "\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83125ad3-d154-4aa7-bc8b-7807eb600d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PRETRAINED_WEIGHTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bbb8b30-9e74-4855-a7a5-444a8f87a85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-01 23:43:07.773832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-01 23:43:07.774646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-01 23:43:07.779346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-01 23:43:07.780129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-01 23:43:07.780882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-01 23:43:07.781623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-01 23:43:07.783026: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-01 23:43:07.785248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-01 23:43:07.786013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-01 23:43:07.786757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-01 23:43:08.148185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-01 23:43:08.148985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-01 23:43:08.149728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-01 23:43:08.150444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22299 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "strategy = tfu.strategy.gpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea4f562-a46e-4b06-9989-06d537f7f124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msirdavidludwig\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/sirdavidludwig/dna-embeddings/runs/10jmy948\" target=\"_blank\">snowy-eon-34</a></strong> to <a href=\"https://wandb.ai/sirdavidludwig/dna-embeddings\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_run = wandb.init(project=\"dna-embeddings\", entity=\"sirdavidludwig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8dee3f1-0827-4008-82fa-b288568608ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wandb.config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"learning_rate_schedule\": 0.95,\n",
    "    \"batch_size\": 256,\n",
    "    \"sequence_length\": 150,\n",
    "    \"kmers\": 3,\n",
    "    \"embed_dim\": 64,\n",
    "    \"num_heads\": 8,\n",
    "    \"enc_stack\": 2,\n",
    "    \"dec_stack\": 2,\n",
    "    \"latent_dim\": 64,\n",
    "    \"prenorm\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e11e9c-f7e1-48fd-a033-4c63ce1004ea",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f140fe9-80b4-4438-bda8-d3436225e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = common.find_shelves(\"./datasets\", \"train\")\n",
    "test_samples = common.find_shelves(\"./datasets\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab5baa23-6d2e-40d4-8b34-79bc05dcdfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(\n",
    "    length=config[\"sequence_length\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    include_quality_scores=False\n",
    ")\n",
    "\n",
    "train_gen = common.DnaSequenceGenerator(train_samples, **args)\n",
    "test_gen = common.DnaSequenceGenerator(test_samples, **args)\n",
    "\n",
    "train_gen.start()\n",
    "test_gen.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91d2900e-d8e9-4f93-89b2-898bdd10dab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-01 23:45:17.632356: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_182\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 256\n",
      "        }\n",
      "        dim {\n",
      "          size: 150\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2022-01-01 23:45:17.640530: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_210\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 256\n",
      "        }\n",
      "        dim {\n",
      "          size: 150\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    }
   ],
   "source": [
    "train = train_gen.as_dist_dataset(strategy)\n",
    "test = test_gen.as_dist_dataset(strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aef882-3f20-4246-8060-e9ac8ababee2",
   "metadata": {},
   "source": [
    "---\n",
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67712585-4b0c-4e82-9b7f-bd123e1528aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "Encoder (Functional)         (None, 64)                498688    \n",
      "_________________________________________________________________\n",
      "Decoder (Functional)         (None, 50, 125)           501821    \n",
      "=================================================================\n",
      "Total params: 1,000,509\n",
      "Trainable params: 1,000,509\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "kmers = config[\"kmers\"]\n",
    "sequence_length = config[\"sequence_length\"]//config[\"kmers\"]\n",
    "num_tokens = 5**config[\"kmers\"]\n",
    "embed_dim = config[\"embed_dim\"]\n",
    "latent_dim = config[\"latent_dim\"]\n",
    "enc_stack = config[\"enc_stack\"]\n",
    "dec_stack = config[\"dec_stack\"]\n",
    "num_heads = config[\"num_heads\"]\n",
    "prenorm = config[\"prenorm\"]\n",
    "\n",
    "with strategy.scope():\n",
    "    # Encoder\n",
    "    y = x = keras.layers.Input((sequence_length,))\n",
    "    \n",
    "    # K-MER encoding\n",
    "    # kmer_powers = np.full(kmers, 5)**np.arange(kmers - 1, -1, -1)\n",
    "    # y = keras.layers.Lambda(lambda x: tf.reduce_sum(tf.reshape(x, (-1, sequence_length, kmers))**kmer_powers, axis=2))(y)\n",
    "    \n",
    "    y = keras.layers.Embedding(input_dim=num_tokens, output_dim=embed_dim)(y)\n",
    "    y = common.FixedPositionEmbedding(length=sequence_length, embed_dim=embed_dim)(y)\n",
    "    for _ in range(enc_stack):\n",
    "        y = common.TransformerBlock(embed_dim, num_heads, ff_dim=embed_dim, prenorm=prenorm)(y)\n",
    "    y = keras.layers.Flatten()(y)\n",
    "    y = keras.layers.Dense(latent_dim)(y)\n",
    "    y = keras.layers.LayerNormalization()(y)\n",
    "    encoder = keras.Model(x, y, name=\"Encoder\")\n",
    "\n",
    "    # Decoder\n",
    "    y = x = keras.layers.Input((encoder.output.shape[1:]))\n",
    "    y = keras.layers.Dense(sequence_length*embed_dim)(y)\n",
    "    y = keras.layers.Reshape((-1, embed_dim))(y)\n",
    "    y = embed = common.FixedPositionEmbedding(length=sequence_length, embed_dim=embed_dim)(y)\n",
    "    for _ in range(dec_stack):\n",
    "        y = common.TransformerBlock(embed_dim, num_heads, ff_dim=embed_dim, prenorm=prenorm)(y)\n",
    "    y = keras.layers.Dense(num_tokens, activation=\"softmax\")(y)\n",
    "    decoder = keras.Model(x, y, name=\"Decoder\")\n",
    "\n",
    "    # Coupled model\n",
    "    y = x = keras.layers.Input(encoder.input.shape[1:])\n",
    "    y = encoder(y)\n",
    "    y = decoder(y)\n",
    "    model = keras.Model(x, y, name=\"Autoencoder\")\n",
    "    model.encoder = encoder\n",
    "    model.decoder = decoder\n",
    "    model.compile(\n",
    "#         optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=0.0001),\n",
    "        keras.optimizers.Nadam(1e-3),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6da9681e-cde7-423f-8d22-5e5314283e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"./models/embed_3mer_64.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c5c0dc6-61d2-4472-bd20-c43033358362",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    loss_obj = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction=\"none\")\n",
    "    metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "    optimizer = keras.optimizers.Adam(config[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "562eb139-4735-491e-85d9-9a24d94d59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_schedule(optimizer, scale = 0.95):\n",
    "    optimizer.learning_rate.assign(optimizer.learning_rate*scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19b30698-5cab-436c-a9a1-966c0852c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(real, pred):\n",
    "    loss = loss_obj(real, pred)\n",
    "    return tf.nn.compute_average_loss(loss, global_batch_size=config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c2b99a5-a4bd-47c8-b6eb-95cf0132c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_kmer_sequence_batch(batch):\n",
    "    kmer_powers = np.full(kmers, 5)**np.arange(kmers - 1, -1, -1)\n",
    "    return tf.reduce_sum(tf.reshape(batch, (batch.shape[0], sequence_length, kmers))*kmer_powers, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1df9bcd-0fbf-4e79-b56d-caf944502da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch):\n",
    "    kmer_batch = encode_kmer_sequence_batch(batch)\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(kmer_batch)\n",
    "        loss = loss_fn(kmer_batch, pred)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    metric.update_state(kmer_batch, pred)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def dist_train_step(batch):\n",
    "    losses = strategy.run(train_step, args=batch)\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.MEAN, losses, axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c54e1b54-19a0-448e-8f33-839a1764d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_step(batch):\n",
    "    kmer_batch = encode_kmer_sequence_batch(batch)\n",
    "    pred = model(kmer_batch)\n",
    "    loss = loss_fn(kmer_batch, pred)\n",
    "    metric.update_state(kmer_batch, pred)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def dist_validate_step(batch):\n",
    "    losses = strategy.run(validate_step, args=batch)\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.MEAN, losses, axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f728d82-33c6-49cf-ae5b-2bbb3e5f35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "losses = []\n",
    "val_losses = []\n",
    "accuracies = []\n",
    "val_accuracies = []\n",
    "step = 0\n",
    "log_frequency = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36e5c0e1-83a5-4022-8184-32df31548e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 completed. loss=2.994931936264038; accuracy=0.8650173544883728; learning rate=1.7384587408741936e-05"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    x_train = iter(train)\n",
    "    x_test = iter(test)\n",
    "    \n",
    "    for step in range(step, step+10000):\n",
    "        batch = next(x_train)\n",
    "        loss = dist_train_step(batch)\n",
    "        \n",
    "        steps.append(step+1)\n",
    "        losses.append(loss.numpy())\n",
    "        accuracies.append(metric.result().numpy())\n",
    "        \n",
    "        metric.reset_states()\n",
    "        \n",
    "        if step > 2000 and step % 100 == 0:\n",
    "            learning_rate_schedule(optimizer, config[\"learning_rate_schedule\"])\n",
    "        \n",
    "        print(f\"\\r{steps[-1]} completed. loss={losses[-1]}; accuracy={accuracies[-1]}; learning rate={optimizer.learning_rate.numpy()}\", end=\"\")\n",
    "        \n",
    "        if step % log_frequency == 0:\n",
    "            batch = next(x_test)\n",
    "            loss = dist_validate_step(batch)\n",
    "            \n",
    "            val_losses.append(loss.numpy())\n",
    "            val_accuracies.append(metric.result().numpy())\n",
    "            metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95a96a31-8726-4c04-b7e2-7c33ed1daa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\n",
    "    \"Loss\": wandb.plot.line_series(\n",
    "        xs=steps[::log_frequency],\n",
    "        ys=[losses[::log_frequency], val_losses],\n",
    "        keys=[\"Training Loss\", \"Validation Loss\"],\n",
    "        title=\"Loss\",\n",
    "        xname=\"Step\"),\n",
    "    \"Accuracy\": wandb.plot.line_series(\n",
    "        xs=steps[::log_frequency],\n",
    "        ys=[accuracies[::log_frequency], val_accuracies],\n",
    "        keys=[\"Training Accuracy\", \"Validation Accuracy\"],\n",
    "        title=\"Accuarcy\",\n",
    "        xname=\"Step\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "040bd20a-e5a3-4ae9-bd99-58795526d8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./models/embed_3mer_64.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed9e1428-b576-4804-bcab-cfb96e5384ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.011043923\n",
      "std: 1.029386\n",
      "shape: (256, 4)\n"
     ]
    }
   ],
   "source": [
    "latent = encoder(encode_kmer_sequence_batch(next(test_gen)[0]))\n",
    "print(\"mean:\", np.mean(latent))\n",
    "print(\"std:\", np.std(latent))\n",
    "print(\"shape:\", latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2eb90661-4144-4b17-8f60-c2aad27928b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2597... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">snowy-eon-34</strong>: <a href=\"https://wandb.ai/sirdavidludwig/dna-embeddings/runs/10jmy948\" target=\"_blank\">https://wandb.ai/sirdavidludwig/dna-embeddings/runs/10jmy948</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220101_234308-10jmy948/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
